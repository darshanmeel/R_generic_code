---
output: word_document
---
```{r}
options(warn=-1)
#load the csv file and provide the name as lv
setwd('/Users/dsing001/myr/R_generic_code/myrpreprocess/R')
library(ggplot2)
library(discretization)
library(randomForest)
library(ROCR)
library(myrpreprocess)
library(unbalanced)
filename <- 'LendingClub.csv'
header <- TRUE
filename

clscol='Class'

#run prelim processing
X <- read.csv(file=filename,header=header)
clscolpos <- match(clscol,colnames(X))
X <- X[complete.cases(X),]
X <- moveclasscolintheend(X,clscolpos)
X[,clscolpos] <- as.character(X[,clscolpos])

X[X[,clscolpos]=='Creditworthy',clscolpos] <- '0'
X[X[,clscolpos]=='Uncreditworthy',clscolpos] <- '1'
X[,clscolpos] <- as.factor(as.numeric(X[,clscol]))



#Now fix all the issues you want to with your data which you might have got idea from Pass_1 as well as from your own analytics.

#Now convert all the factor columns to ordered in final model these wont be ordered
factcols <- findallfactorcols(X[,-ncol(X)])
print(factcols)
for (cl in factcols){
  ab <- as.data.frame(table(X[,cl]))
  ab <- ab[order(ab[,2]),]
  nwlvls <- paste(ab[,1],sep=',')
  nwlvls <- as.data.frame(nwlvls)
  nwlvls <- rownames(nwlvls)
  ab <- cbind(ab,nwlvls)
  print (ab)
  X[,cl] <- as.character(X[,cl])
  cl_vc <- X[,cl]
  X[,cl] <- as.numeric(unlist(lapply(cl_vc,function(vl) ab[ab[,1]==vl,3])))
  X[,cl] <- factor(X[,cl], levels = nwlvls)
}

#convert some num cols to factor columns as ordered factor columns
numcols_to_be_converted <- c(3,8,11,12,13,14,17,18)
X[,numcols_to_be_converted ] <- lapply(X[,numcols_to_be_converted], function(cl) as.ordered(round(cl)))

str(X)
# Now break the data into 3 parts, train data, test data and valid data as feature selection should always be done on train data and never on test and valid data.
#valid ratio 0.3 means it will be 30 % of data remaning after test_ratio has been splitted. so if test_ratio is 0.3 and valid_ratio is 0.3  valid data will be (1-0.3)*0.3 i.e. 0.21 of total data passed
dt_set <- generte_train_test_validset(X,test_ratio=0.3,valid_ratio=0.1)
train_data <- dt_set$train_data
test_data <- dt_set$test_data
valid_data <- dt_set$valid_data


# see whether new cols are converted to fact columns.
str(X)

#Now merege the ordered lvl data

#train_data <- dt_set$train_data
train_data <- dt_set$train_data
test_data <- dt_set$test_data
valid_data <- dt_set$valid_data
td <- merge_lvls(train_data,test_data,valid_data)
train_data <- td[[1]]
test_data <- td[[2]]
valid_data <- td[[3]]

dim(train_data)
str(train_data)
# Now merge all unordered data
#train_data <- dt_set$train_data
td <- merge_lvls(train_data,test_data,valid_data)
train_data <- td[[1]]
test_data <- td[[2]]
valid_data <- td[[3]]

str(train_data)




write.csv(train_data,'fs_train_data.csv',row.names=FALSE,quote=TRUE)
write.csv(test_data,'fs_test_data.csv',row.names=FALSE,quote=TRUE)
write.csv(valid_data,'fs_validdata.csv',row.names=FALSE,quote=TRUE)

#Generate graph. It might take long time s there are 120 columns and thus could belots of graphs

#plotvariousgraphs(train_data,individualplots=TRUE,crossplot=TRUE,byfactcols=FALSE,plotall=FALSE,clscol=clscol,timetovieweachinput=0)

#
# Now run the pass 2 and you will see which features to use.
# Here k means how many parameters to bring
# ranking means what ranking to use for random forest. You can pass random forest parameters as well.
#findusefulfeatures(train_data,alpha = 0.05,ranking='accuracy',k=20,frml1,ntree=10)

options(warn=0)
```

